# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, The QEMU Project Developers
# This file is distributed under the same license as the QEMU package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: QEMU 10.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-28 10:00+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../devel/atomics.rst:5
msgid "Atomic operations in QEMU"
msgstr ""

#: ../../../devel/atomics.rst:7
msgid ""
"CPUs perform independent memory operations effectively in random order. but "
"this can be a problem for CPU-CPU interaction (including interactions "
"between QEMU and the guest).  Multi-threaded programs use various tools to "
"instruct the compiler and the CPU to restrict the order to something that is "
"consistent with the expectations of the programmer."
msgstr ""

#: ../../../devel/atomics.rst:13
msgid ""
"The most basic tool is locking.  Mutexes, condition variables and semaphores "
"are used in QEMU, and should be the default approach to synchronization.  "
"Anything else is considerably harder, but it's also justified more often "
"than one would like; the most performance-critical parts of QEMU in "
"particular require a very low level approach to concurrency, involving "
"memory barriers and atomic operations.  The semantics of concurrent memory "
"accesses are governed by the C11 memory model."
msgstr ""

#: ../../../devel/atomics.rst:22
msgid ""
"QEMU provides a header, ``qemu/atomic.h``, which wraps C11 atomics to "
"provide better portability and a less verbose syntax.  ``qemu/atomic.h`` "
"provides macros that fall in three camps:"
msgstr ""

#: ../../../devel/atomics.rst:26
msgid "compiler barriers: ``barrier()``;"
msgstr ""

#: ../../../devel/atomics.rst:28
msgid ""
"weak atomic access and manual memory barriers: ``qatomic_read()``, "
"``qatomic_set()``, ``smp_rmb()``, ``smp_wmb()``, ``smp_mb()``, "
"``smp_mb_acquire()``, ``smp_mb_release()``, ``smp_read_barrier_depends()``, "
"``smp_mb__before_rmw()``, ``smp_mb__after_rmw()``;"
msgstr ""

#: ../../../devel/atomics.rst:33
msgid "sequentially consistent atomic access: everything else."
msgstr ""

#: ../../../devel/atomics.rst:35
msgid ""
"In general, use of ``qemu/atomic.h`` should be wrapped with more easily used "
"data structures (e.g. the lock-free singly-linked list operations "
"``QSLIST_INSERT_HEAD_ATOMIC`` and ``QSLIST_MOVE_ATOMIC``) or synchronization "
"primitives (such as RCU, ``QemuEvent`` or ``QemuLockCnt``).  Bare use of "
"atomic operations and memory barriers should be limited to inter-thread "
"checking of flags and documented thoroughly."
msgstr ""

#: ../../../devel/atomics.rst:45
msgid "Compiler memory barrier"
msgstr "編譯器記憶體屏障"

#: ../../../devel/atomics.rst:47
msgid ""
"``barrier()`` prevents the compiler from moving the memory accesses on "
"either side of it to the other side.  The compiler barrier has no direct "
"effect on the CPU, which may then reorder things however it wishes."
msgstr ""

#: ../../../devel/atomics.rst:51
msgid ""
"``barrier()`` is mostly used within ``qemu/atomic.h`` itself.  On some "
"architectures, CPU guarantees are strong enough that blocking compiler "
"optimizations already ensures the correct order of execution.  In this case, "
"``qemu/atomic.h`` will reduce stronger memory barriers to simple compiler "
"barriers."
msgstr ""

#: ../../../devel/atomics.rst:57
msgid ""
"Still, ``barrier()`` can be useful when writing code that can be interrupted "
"by signal handlers."
msgstr ""

#: ../../../devel/atomics.rst:62
msgid "Sequentially consistent atomic access"
msgstr ""

#: ../../../devel/atomics.rst:64
msgid ""
"Most of the operations in the ``qemu/atomic.h`` header ensure *sequential "
"consistency*, where \"the result of any execution is the same as if the "
"operations of all the processors were executed in some sequential order, and "
"the operations of each individual processor appear in this sequence in the "
"order specified by its program\"."
msgstr ""

#: ../../../devel/atomics.rst:70
msgid ""
"``qemu/atomic.h`` provides the following set of atomic read-modify-write "
"operations::"
msgstr ""

#: ../../../devel/atomics.rst:73
msgid ""
"void qatomic_inc(ptr)\n"
"void qatomic_dec(ptr)\n"
"void qatomic_add(ptr, val)\n"
"void qatomic_sub(ptr, val)\n"
"void qatomic_and(ptr, val)\n"
"void qatomic_or(ptr, val)\n"
"\n"
"typeof(*ptr) qatomic_fetch_inc(ptr)\n"
"typeof(*ptr) qatomic_fetch_dec(ptr)\n"
"typeof(*ptr) qatomic_fetch_add(ptr, val)\n"
"typeof(*ptr) qatomic_fetch_sub(ptr, val)\n"
"typeof(*ptr) qatomic_fetch_and(ptr, val)\n"
"typeof(*ptr) qatomic_fetch_or(ptr, val)\n"
"typeof(*ptr) qatomic_fetch_xor(ptr, val)\n"
"typeof(*ptr) qatomic_fetch_inc_nonzero(ptr)\n"
"typeof(*ptr) qatomic_xchg(ptr, val)\n"
"typeof(*ptr) qatomic_cmpxchg(ptr, old, new)"
msgstr ""

#: ../../../devel/atomics.rst:91
msgid ""
"all of which return the old value of ``*ptr``.  These operations are "
"polymorphic; they operate on any type that is as wide as a pointer or "
"smaller."
msgstr ""

#: ../../../devel/atomics.rst:95
msgid "Similar operations return the new value of ``*ptr``::"
msgstr ""

#: ../../../devel/atomics.rst:97
msgid ""
"typeof(*ptr) qatomic_inc_fetch(ptr)\n"
"typeof(*ptr) qatomic_dec_fetch(ptr)\n"
"typeof(*ptr) qatomic_add_fetch(ptr, val)\n"
"typeof(*ptr) qatomic_sub_fetch(ptr, val)\n"
"typeof(*ptr) qatomic_and_fetch(ptr, val)\n"
"typeof(*ptr) qatomic_or_fetch(ptr, val)\n"
"typeof(*ptr) qatomic_xor_fetch(ptr, val)"
msgstr ""

#: ../../../devel/atomics.rst:105
msgid ""
"``qemu/atomic.h`` also provides an optimized shortcut for ``qatomic_set`` "
"followed by ``smp_mb``::"
msgstr ""

#: ../../../devel/atomics.rst:108
msgid "void         qatomic_set_mb(ptr, val)"
msgstr ""

#: ../../../devel/atomics.rst:112
msgid "Weak atomic access and manual memory barriers"
msgstr ""

#: ../../../devel/atomics.rst:114
msgid ""
"Compared to sequentially consistent atomic access, programming with weaker "
"consistency models can be considerably more complicated. The only guarantees "
"that you can rely upon in this case are:"
msgstr ""

#: ../../../devel/atomics.rst:118
msgid ""
"atomic accesses will not cause data races (and hence undefined behavior); "
"ordinary accesses instead cause data races if they are concurrent with other "
"accesses of which at least one is a write.  In order to ensure this, the "
"compiler will not optimize accesses out of existence, create unsolicited "
"accesses, or perform other similar optimizations."
msgstr ""

#: ../../../devel/atomics.rst:124
msgid ""
"acquire operations will appear to happen, with respect to the other "
"components of the system, before all the LOAD or STORE operations specified "
"afterwards."
msgstr ""

#: ../../../devel/atomics.rst:128
msgid ""
"release operations will appear to happen, with respect to the other "
"components of the system, after all the LOAD or STORE operations specified "
"before."
msgstr ""

#: ../../../devel/atomics.rst:132
msgid ""
"release operations will *synchronize with* acquire operations; see :ref:"
"`acqrel` for a detailed explanation."
msgstr ""

#: ../../../devel/atomics.rst:135
msgid "When using this model, variables are accessed with:"
msgstr ""

#: ../../../devel/atomics.rst:137
msgid ""
"``qatomic_read()`` and ``qatomic_set()``; these prevent the compiler from "
"optimizing accesses out of existence and creating unsolicited accesses, but "
"do not otherwise impose any ordering on loads and stores: both the compiler "
"and the processor are free to reorder them."
msgstr ""

#: ../../../devel/atomics.rst:143
msgid ""
"``qatomic_load_acquire()``, which guarantees the LOAD to appear to happen, "
"with respect to the other components of the system, before all the LOAD or "
"STORE operations specified afterwards. Operations coming before "
"``qatomic_load_acquire()`` can still be reordered after it."
msgstr ""

#: ../../../devel/atomics.rst:149
msgid ""
"``qatomic_store_release()``, which guarantees the STORE to appear to happen, "
"with respect to the other components of the system, after all the LOAD or "
"STORE operations specified before. Operations coming after "
"``qatomic_store_release()`` can still be reordered before it."
msgstr ""

#: ../../../devel/atomics.rst:155
msgid ""
"Restrictions to the ordering of accesses can also be specified using the "
"memory barrier macros: ``smp_rmb()``, ``smp_wmb()``, ``smp_mb()``, "
"``smp_mb_acquire()``, ``smp_mb_release()``, ``smp_read_barrier_depends()``."
msgstr ""

#: ../../../devel/atomics.rst:159
msgid ""
"Memory barriers control the order of references to shared memory. They come "
"in six kinds:"
msgstr ""

#: ../../../devel/atomics.rst:162
msgid ""
"``smp_rmb()`` guarantees that all the LOAD operations specified before the "
"barrier will appear to happen before all the LOAD operations specified after "
"the barrier with respect to the other components of the system."
msgstr ""

#: ../../../devel/atomics.rst:167
msgid ""
"In other words, ``smp_rmb()`` puts a partial ordering on loads, but is not "
"required to have any effect on stores."
msgstr ""

#: ../../../devel/atomics.rst:170
msgid ""
"``smp_wmb()`` guarantees that all the STORE operations specified before the "
"barrier will appear to happen before all the STORE operations specified "
"after the barrier with respect to the other components of the system."
msgstr ""

#: ../../../devel/atomics.rst:175
msgid ""
"In other words, ``smp_wmb()`` puts a partial ordering on stores, but is not "
"required to have any effect on loads."
msgstr ""

#: ../../../devel/atomics.rst:178
msgid ""
"``smp_mb_acquire()`` guarantees that all the LOAD operations specified "
"before the barrier will appear to happen before all the LOAD or STORE "
"operations specified after the barrier with respect to the other components "
"of the system."
msgstr ""

#: ../../../devel/atomics.rst:183
msgid ""
"``smp_mb_release()`` guarantees that all the STORE operations specified "
"*after* the barrier will appear to happen after all the LOAD or STORE "
"operations specified *before* the barrier with respect to the other "
"components of the system."
msgstr ""

#: ../../../devel/atomics.rst:188
msgid ""
"``smp_mb()`` guarantees that all the LOAD and STORE operations specified "
"before the barrier will appear to happen before all the LOAD and STORE "
"operations specified after the barrier with respect to the other components "
"of the system."
msgstr ""

#: ../../../devel/atomics.rst:193
msgid ""
"``smp_mb()`` puts a partial ordering on both loads and stores.  It is "
"stronger than both a read and a write memory barrier; it implies both "
"``smp_mb_acquire()`` and ``smp_mb_release()``, but it also prevents STOREs "
"coming before the barrier from overtaking LOADs coming after the barrier and "
"vice versa."
msgstr ""

#: ../../../devel/atomics.rst:199
msgid ""
"``smp_read_barrier_depends()`` is a weaker kind of read barrier.  On most "
"processors, whenever two loads are performed such that the second depends on "
"the result of the first (e.g., the first load retrieves the address to which "
"the second load will be directed), the processor will guarantee that the "
"first LOAD will appear to happen before the second with respect to the other "
"components of the system. Therefore, unlike ``smp_rmb()`` or "
"``qatomic_load_acquire()``, ``smp_read_barrier_depends()`` can be just a "
"compiler barrier on weakly-ordered architectures such as Arm or PPC\\ "
"[#alpha]_."
msgstr ""

#: ../../../devel/atomics.rst:209
msgid ""
"Note that the first load really has to have a _data_ dependency and not a "
"control dependency.  If the address for the second load is dependent on the "
"first load, but the dependency is through a conditional rather than actually "
"loading the address itself, then it's a _control_ dependency and a full read "
"barrier or better is required."
msgstr ""

#: ../../../devel/atomics.rst:215
msgid ""
"The DEC Alpha is an exception, because ``smp_read_barrier_depends()`` needs "
"a processor barrier.  On strongly-ordered architectures such as x86 or s390, "
"``smp_rmb()`` and ``qatomic_load_acquire()`` can also be compiler barriers "
"only."
msgstr ""

#: ../../../devel/atomics.rst:220
msgid ""
"Memory barriers and ``qatomic_load_acquire``/``qatomic_store_release`` are "
"mostly used when a data structure has one thread that is always a writer and "
"one thread that is always a reader:"
msgstr ""

#: ../../../devel/atomics.rst:225 ../../../devel/atomics.rst:337
#: ../../../devel/atomics.rst:362 ../../../devel/atomics.rst:380
#: ../../../devel/atomics.rst:404
msgid "thread 1"
msgstr ""

#: ../../../devel/atomics.rst:225 ../../../devel/atomics.rst:337
#: ../../../devel/atomics.rst:362 ../../../devel/atomics.rst:380
#: ../../../devel/atomics.rst:404
msgid "thread 2"
msgstr ""

#: ../../../devel/atomics.rst:229
msgid ""
"qatomic_store_release(&a, x);\n"
"qatomic_store_release(&b, y);"
msgstr ""

#: ../../../devel/atomics.rst:229
msgid ""
"y = qatomic_load_acquire(&b);\n"
"x = qatomic_load_acquire(&a);"
msgstr ""

#: ../../../devel/atomics.rst:233
msgid ""
"In this case, correctness is easy to check for using the \"pairing\" trick "
"that is explained below."
msgstr ""

#: ../../../devel/atomics.rst:236
msgid ""
"Sometimes, a thread is accessing many variables that are otherwise unrelated "
"to each other (for example because, apart from the current thread, exactly "
"one other thread will read or write each of these variables).  In this case, "
"it is possible to \"hoist\" the barriers outside a loop.  For example:"
msgstr ""

#: ../../../devel/atomics.rst:243 ../../../devel/atomics.rst:262
#: ../../../devel/atomics.rst:279
msgid "before"
msgstr ""

#: ../../../devel/atomics.rst:243 ../../../devel/atomics.rst:262
#: ../../../devel/atomics.rst:279
msgid "after"
msgstr ""

#: ../../../devel/atomics.rst:247
msgid ""
"n = 0;\n"
"for (i = 0; i < 10; i++)\n"
"  n += qatomic_load_acquire(&a[i]);"
msgstr ""

#: ../../../devel/atomics.rst:247
msgid ""
"n = 0;\n"
"for (i = 0; i < 10; i++)\n"
"  n += qatomic_read(&a[i]);\n"
"smp_mb_acquire();"
msgstr ""

#: ../../../devel/atomics.rst:255
msgid ""
"for (i = 0; i < 10; i++)\n"
"  qatomic_store_release(&a[i], false);"
msgstr ""

#: ../../../devel/atomics.rst:254
msgid ""
"smp_mb_release();\n"
"for (i = 0; i < 10; i++)\n"
"  qatomic_set(&a[i], false);"
msgstr ""

#: ../../../devel/atomics.rst:259
msgid "Splitting a loop can also be useful to reduce the number of barriers:"
msgstr ""

#: ../../../devel/atomics.rst:266
msgid ""
"n = 0;\n"
"for (i = 0; i < 10; i++) {\n"
"  qatomic_store_release(&a[i], false);\n"
"  smp_mb();\n"
"  n += qatomic_read(&b[i]);\n"
"}"
msgstr ""

#: ../../../devel/atomics.rst:266
msgid ""
"smp_mb_release();\n"
"for (i = 0; i < 10; i++)\n"
"  qatomic_set(&a[i], false);\n"
"smb_mb();\n"
"n = 0;\n"
"for (i = 0; i < 10; i++)\n"
"  n += qatomic_read(&b[i]);"
msgstr ""

#: ../../../devel/atomics.rst:275
msgid ""
"In this case, a ``smp_mb_release()`` is also replaced with a (possibly "
"cheaper, and clearer as well) ``smp_wmb()``:"
msgstr ""

#: ../../../devel/atomics.rst:284
msgid ""
"for (i = 0; i < 10; i++) {\n"
"  qatomic_store_release(&a[i], false);\n"
"  qatomic_store_release(&b[i], false);\n"
"}"
msgstr ""

#: ../../../devel/atomics.rst:283
msgid ""
"smp_mb_release();\n"
"for (i = 0; i < 10; i++)\n"
"  qatomic_set(&a[i], false);\n"
"smb_wmb();\n"
"for (i = 0; i < 10; i++)\n"
"  qatomic_set(&b[i], false);"
msgstr ""

#: ../../../devel/atomics.rst:295
msgid "Acquire/release pairing and the *synchronizes-with* relation"
msgstr ""

#: ../../../devel/atomics.rst:297
msgid ""
"Atomic operations other than ``qatomic_set()`` and ``qatomic_read()`` have "
"either *acquire* or *release* semantics\\ [#rmw]_.  This has two effects:"
msgstr ""

#: ../../../devel/atomics.rst:300
msgid ""
"Read-modify-write operations can have both---acquire applies to the read "
"part, and release to the write."
msgstr ""

#: ../../../devel/atomics.rst:303
msgid ""
"within a thread, they are ordered either before subsequent operations (for "
"acquire) or after previous operations (for release)."
msgstr ""

#: ../../../devel/atomics.rst:306
msgid ""
"if a release operation in one thread *synchronizes with* an acquire "
"operation in another thread, the ordering constraints propagates from the "
"first to the second thread.  That is, everything before the release "
"operation in the first thread is guaranteed to *happen before* everything "
"after the acquire operation in the second thread."
msgstr ""

#: ../../../devel/atomics.rst:312
msgid ""
"The concept of acquire and release semantics is not exclusive to atomic "
"operations; almost all higher-level synchronization primitives also have "
"acquire or release semantics.  For example:"
msgstr ""

#: ../../../devel/atomics.rst:316
msgid ""
"``pthread_mutex_lock`` has acquire semantics, ``pthread_mutex_unlock`` has "
"release semantics and synchronizes with a ``pthread_mutex_lock`` for the "
"same mutex."
msgstr ""

#: ../../../devel/atomics.rst:320
msgid ""
"``pthread_cond_signal`` and ``pthread_cond_broadcast`` have release "
"semantics; ``pthread_cond_wait`` has both release semantics (synchronizing "
"with ``pthread_mutex_lock``) and acquire semantics (synchronizing with "
"``pthread_mutex_unlock`` and signaling of the condition variable)."
msgstr ""

#: ../../../devel/atomics.rst:325
msgid ""
"``pthread_create`` has release semantics and synchronizes with the start of "
"the new thread; ``pthread_join`` has acquire semantics and synchronizes with "
"the exiting of the thread."
msgstr ""

#: ../../../devel/atomics.rst:329
msgid ""
"``qemu_event_set`` has release semantics, ``qemu_event_wait`` has acquire "
"semantics."
msgstr ""

#: ../../../devel/atomics.rst:332
msgid ""
"For example, in the following example there are no atomic accesses, but "
"still thread 2 is relying on the *synchronizes-with* relation between "
"``pthread_exit`` (release) and ``pthread_join`` (acquire):"
msgstr ""

#: ../../../devel/atomics.rst:341
msgid ""
"*a = 1;\n"
"pthread_exit(a);"
msgstr ""

#: ../../../devel/atomics.rst:342
msgid ""
"pthread_join(thread1, &a);\n"
"x = *a;"
msgstr ""

#: ../../../devel/atomics.rst:346
msgid ""
"Synchronization between threads basically descends from this pairing of a "
"release operation and an acquire operation.  Therefore, atomic operations "
"other than ``qatomic_set()`` and ``qatomic_read()`` will almost always be "
"paired with another operation of the opposite kind: an acquire operation "
"will pair with a release operation and vice versa.  This rule of thumb is "
"extremely useful; in the case of QEMU, however, note that the other "
"operation may actually be in a driver that runs in the guest!"
msgstr ""

#: ../../../devel/atomics.rst:354
msgid ""
"``smp_read_barrier_depends()``, ``smp_rmb()``, ``smp_mb_acquire()``, "
"``qatomic_load_acquire()`` and ``qatomic_rcu_read()`` all count as acquire "
"operations.  ``smp_wmb()``, ``smp_mb_release()``, "
"``qatomic_store_release()`` and ``qatomic_rcu_set()`` all count as release "
"operations.  ``smp_mb()`` counts as both acquire and release, therefore it "
"can pair with any other atomic operation.  Here is an example:"
msgstr ""

#: ../../../devel/atomics.rst:366
msgid ""
"qatomic_set(&a, 1);\n"
"smp_wmb();\n"
"qatomic_set(&b, 2);"
msgstr ""

#: ../../../devel/atomics.rst:368
msgid ""
"x = qatomic_read(&b);\n"
"smp_rmb();\n"
"y = qatomic_read(&a);"
msgstr ""

#: ../../../devel/atomics.rst:373
msgid ""
"Note that a load-store pair only counts if the two operations access the "
"same variable: that is, a store-release on a variable ``x`` *synchronizes "
"with* a load-acquire on a variable ``x``, while a release barrier "
"synchronizes with any acquire operation.  The following example shows "
"correct synchronization:"
msgstr ""

#: ../../../devel/atomics.rst:384
msgid ""
"qatomic_set(&a, 1);\n"
"qatomic_store_release(&b, 2);"
msgstr ""

#: ../../../devel/atomics.rst:385
msgid ""
"x = qatomic_load_acquire(&b);\n"
"y = qatomic_read(&a);"
msgstr ""

#: ../../../devel/atomics.rst:389
msgid ""
"Acquire and release semantics of higher-level primitives can also be relied "
"upon for the purpose of establishing the *synchronizes with* relation."
msgstr ""

#: ../../../devel/atomics.rst:393
msgid ""
"Note that the \"writing\" thread is accessing the variables in the opposite "
"order as the \"reading\" thread.  This is expected: stores before a release "
"operation will normally match the loads after the acquire operation, and "
"vice versa.  In fact, this happened already in the ``pthread_exit``/"
"``pthread_join`` example above."
msgstr ""

#: ../../../devel/atomics.rst:399
msgid ""
"Finally, this more complex example has more than two accesses and data "
"dependency barriers.  It also does not use atomic accesses whenever there "
"cannot be a data race:"
msgstr ""

#: ../../../devel/atomics.rst:408
msgid ""
"b[2] = 1;\n"
"smp_wmb();\n"
"x->i = 2;\n"
"smp_wmb();\n"
"qatomic_set(&a, x);"
msgstr ""

#: ../../../devel/atomics.rst:412
msgid ""
"x = qatomic_read(&a);\n"
"smp_read_barrier_depends();\n"
"y = x->i;\n"
"smp_read_barrier_depends();\n"
"z = b[y];"
msgstr ""

#: ../../../devel/atomics.rst:420
msgid "Comparison with Linux kernel primitives"
msgstr ""

#: ../../../devel/atomics.rst:422
msgid ""
"Here is a list of differences between Linux kernel atomic operations and "
"memory barriers, and the equivalents in QEMU:"
msgstr ""

#: ../../../devel/atomics.rst:425
msgid ""
"atomic operations in Linux are always on a 32-bit int type and use a boxed "
"``atomic_t`` type; atomic operations in QEMU are polymorphic and use normal "
"C types."
msgstr ""

#: ../../../devel/atomics.rst:429
msgid ""
"Originally, ``atomic_read`` and ``atomic_set`` in Linux gave no guarantee at "
"all. Linux 4.1 updated them to implement volatile semantics via "
"``ACCESS_ONCE`` (or the more recent ``READ``/``WRITE_ONCE``)."
msgstr ""

#: ../../../devel/atomics.rst:433
msgid ""
"QEMU's ``qatomic_read`` and ``qatomic_set`` implement C11 atomic relaxed "
"semantics if the compiler supports it, and volatile semantics otherwise. "
"Both semantics prevent the compiler from doing certain transformations; the "
"difference is that atomic accesses are guaranteed to be atomic, while "
"volatile accesses aren't. Thus, in the volatile case we just cross our "
"fingers hoping that the compiler will generate atomic accesses, since we "
"assume the variables passed are machine-word sized and properly aligned."
msgstr ""

#: ../../../devel/atomics.rst:442
msgid ""
"No barriers are implied by ``qatomic_read`` and ``qatomic_set`` in either "
"Linux or QEMU."
msgstr ""

#: ../../../devel/atomics.rst:445
msgid "atomic read-modify-write operations in Linux are of three kinds:"
msgstr ""

#: ../../../devel/atomics.rst:448
msgid "``atomic_OP``"
msgstr ""

#: ../../../devel/atomics.rst:448
msgid "returns void"
msgstr ""

#: ../../../devel/atomics.rst:449
msgid "``atomic_OP_return``"
msgstr ""

#: ../../../devel/atomics.rst:449
msgid "returns new value of the variable"
msgstr ""

#: ../../../devel/atomics.rst:450
msgid "``atomic_fetch_OP``"
msgstr ""

#: ../../../devel/atomics.rst:450 ../../../devel/atomics.rst:451
msgid "returns the old value of the variable"
msgstr ""

#: ../../../devel/atomics.rst:451
msgid "``atomic_cmpxchg``"
msgstr ""

#: ../../../devel/atomics.rst:454
msgid "In QEMU, the second kind is named ``atomic_OP_fetch``."
msgstr ""

#: ../../../devel/atomics.rst:456
msgid ""
"different atomic read-modify-write operations in Linux imply a different set "
"of memory barriers. In QEMU, all of them enforce sequential consistency: "
"there is a single order in which the program sees them happen."
msgstr ""

#: ../../../devel/atomics.rst:461
msgid ""
"however, according to the C11 memory model that QEMU uses, this order does "
"not propagate to other memory accesses on either side of the read-modify-"
"write operation.  As far as those are concerned, the operation consist of "
"just a load-acquire followed by a store-release. Stores that precede the RMW "
"operation, and loads that follow it, can still be reordered and will happen "
"*in the middle* of the read-modify-write operation!"
msgstr ""

#: ../../../devel/atomics.rst:469
msgid "Therefore, the following example is correct in Linux but not in QEMU:"
msgstr ""

#: ../../../devel/atomics.rst:472
msgid "Linux (correct)"
msgstr ""

#: ../../../devel/atomics.rst:472
msgid "QEMU (incorrect)"
msgstr ""

#: ../../../devel/atomics.rst:476
msgid ""
"a = atomic_fetch_add(&x, 2);\n"
"b = READ_ONCE(&y);"
msgstr ""

#: ../../../devel/atomics.rst:476
msgid ""
"a = qatomic_fetch_add(&x, 2);\n"
"b = qatomic_read(&y);"
msgstr ""

#: ../../../devel/atomics.rst:480
msgid ""
"because the read of ``y`` can be moved (by either the processor or the "
"compiler) before the write of ``x``."
msgstr ""

#: ../../../devel/atomics.rst:483
msgid ""
"Fixing this requires a full memory barrier between the write of ``x`` and "
"the read of ``y``.  QEMU provides ``smp_mb__before_rmw()`` and "
"``smp_mb__after_rmw()``; they act both as an optimization, avoiding the "
"memory barrier on processors where it is unnecessary, and as a clarification "
"of this corner case of the C11 memory model:"
msgstr ""

#: ../../../devel/atomics.rst:490 ../../../devel/atomics.rst:503
msgid "QEMU (correct)"
msgstr ""

#: ../../../devel/atomics.rst:494
msgid ""
"a = qatomic_fetch_add(&x, 2);\n"
"smp_mb__after_rmw();\n"
"b = qatomic_read(&y);"
msgstr ""

#: ../../../devel/atomics.rst:499
msgid ""
"In the common case where only one thread writes ``x``, it is also possible "
"to write it like this:"
msgstr ""

#: ../../../devel/atomics.rst:507
msgid ""
"a = qatomic_read(&x);\n"
"qatomic_set_mb(&x, a + 2);\n"
"b = qatomic_read(&y);"
msgstr ""

#: ../../../devel/atomics.rst:513
msgid "Sources"
msgstr ""

#: ../../../devel/atomics.rst:515
msgid "``Documentation/memory-barriers.txt`` from the Linux kernel"
msgstr ""
